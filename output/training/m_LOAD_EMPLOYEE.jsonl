{"prompt": "# Mapping: m_LOAD_EMPLOYEE\n**Folder:** SampleFolder\n**Description:** \n\n\n## \ud83d\udcc2 Source Definitions\n### EMPLOYEE_SOURCE (Oracle)\n- `EMP_ID`: number (10,0)\n- `EMP_NAME`: varchar2 (50,0)\n\n## \ud83c\udfaf Target Definitions\n### EMPLOYEE_TARGET (Oracle)\n- `EMP_ID`: number (10,0)\n- `EMP_NAME`: varchar2 (50,0)\n\n## \ud83d\udd04 Transformations\n\n### SQ_EMPLOYEE (Source Qualifier)\n**Fields by Group:**\n\n#### Group: default\n- `EMP_ID` (number, INPUT/OUTPUT) \u2192 No logic (Precision: 10, Scale: 0)\n- `EMP_NAME` (varchar2, INPUT/OUTPUT) \u2192 No logic (Precision: 50, Scale: 0)\n\n### EXP_EMPLOYEE (Expression)\n**Fields by Group:**\n\n#### Group: default\n- `EMP_ID` (number, INPUT/OUTPUT) \u2192 No logic (Precision: 10, Scale: 0)\n- `EMP_NAME` (varchar2, INPUT/OUTPUT) \u2192 No logic (Precision: 50, Scale: 0)\n\n## \ud83d\udd01 Connector Flow\n- `EMPLOYEE_SOURCE`.`EMP_ID` \u279d `SQ_EMPLOYEE`.`EMP_ID`\n- `EMPLOYEE_SOURCE`.`EMP_NAME` \u279d `SQ_EMPLOYEE`.`EMP_NAME`\n- `SQ_EMPLOYEE`.`EMP_ID` \u279d `EXP_EMPLOYEE`.`EMP_ID`\n- `SQ_EMPLOYEE`.`EMP_NAME` \u279d `EXP_EMPLOYEE`.`EMP_NAME`\n- `EXP_EMPLOYEE`.`EMP_ID` \u279d `EMPLOYEE_TARGET`.`EMP_ID`\n- `EXP_EMPLOYEE`.`EMP_NAME` \u279d `EMPLOYEE_TARGET`.`EMP_NAME`", "completion": "\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"m_LOAD_EMPLOYEE\") \\\n    .config(\"spark.jars\", \"/path/to/ojdbc8.jar\") \\  # Oracle JDBC jar\n    .getOrCreate()\n\n# JDBC connection properties\noracle_url = \"jdbc:oracle:thin:@//hostname:port/service_name\"\nsource_properties = {\n    \"user\": \"your_source_user\",\n    \"password\": \"your_password\",\n    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n}\n\ntarget_properties = {\n    \"user\": \"your_target_user\",\n    \"password\": \"your_password\",\n    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n}\n\n# \ud83d\udd0d Read from EMPLOYEE_SOURCE\nemployee_df = spark.read.jdbc(\n    url=oracle_url,\n    table=\"EMPLOYEE_SOURCE\",\n    properties=source_properties\n)\n\n# \ud83e\uddea Source Qualifier Transformation (SQ_EMPLOYEE)\n# In this case, it's just passing through the columns.\nsq_employee_df = employee_df.select(\"EMP_ID\", \"EMP_NAME\")\n\n# \ud83e\udde0 Expression Transformation (EXP_EMPLOYEE)\n# No logic applied here, but if needed you could add expressions here.\nexp_employee_df = sq_employee_df.select(\"EMP_ID\", \"EMP_NAME\")\n\n# \ud83c\udfaf Write to EMPLOYEE_TARGET\nexp_employee_df.write.jdbc(\n    url=oracle_url,\n    table=\"EMPLOYEE_TARGET\",\n    mode=\"overwrite\",  # or \"append\", depending on requirements\n    properties=target_properties\n)\n\n# Stop Spark session\nspark.stop()\n```"}
