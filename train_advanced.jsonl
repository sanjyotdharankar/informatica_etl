{"prompt": "Mapping: m_LOAD_EMPLOYEE\nSource: EMPLOYEE_SOURCE\nTarget: EMPLOYEE_TARGET\nSource Fields:\n- `EMP_ID`: number (10,0)\n- `EMP_NAME`: varchar2 (50,0)\nTarget Fields:\n- `EMP_ID`: number (10,0)\n- `EMP_NAME`: varchar2 (50,0)\nTransformations:\nConnector Flow:\n- EMPLOYEE_SOURCE`.`EMP_ID -> SQ_EMPLOYEE\n- EMPLOYEE_SOURCE`.`EMP_NAME -> SQ_EMPLOYEE\n- SQ_EMPLOYEE`.`EMP_ID -> EXP_EMPLOYEE\n- SQ_EMPLOYEE`.`EMP_NAME -> EXP_EMPLOYEE\n- EXP_EMPLOYEE`.`EMP_ID -> EMPLOYEE_TARGET\n- EXP_EMPLOYEE`.`EMP_NAME -> EMPLOYEE_TARGET", "completion": "\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"m_LOAD_EMPLOYEE\") \\\n    .config(\"spark.jars\", \"/path/to/ojdbc8.jar\") \\  # Oracle JDBC jar\n    .getOrCreate()\n\n# JDBC connection properties\noracle_url = \"jdbc:oracle:thin:@//hostname:port/service_name\"\nsource_properties = {\n    \"user\": \"your_source_user\",\n    \"password\": \"your_password\",\n    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n}\n\ntarget_properties = {\n    \"user\": \"your_target_user\",\n    \"password\": \"your_password\",\n    \"driver\": \"oracle.jdbc.driver.OracleDriver\"\n}\n\n# \ud83d\udd0d Read from EMPLOYEE_SOURCE\nemployee_df = spark.read.jdbc(\n    url=oracle_url,\n    table=\"EMPLOYEE_SOURCE\",\n    properties=source_properties\n)\n\n# \ud83e\uddea Source Qualifier Transformation (SQ_EMPLOYEE)\n# In this case, it's just passing through the columns.\nsq_employee_df = employee_df.select(\"EMP_ID\", \"EMP_NAME\")\n\n# \ud83e\udde0 Expression Transformation (EXP_EMPLOYEE)\n# No logic applied here, but if needed you could add expressions here.\nexp_employee_df = sq_employee_df.select(\"EMP_ID\", \"EMP_NAME\")\n\n# \ud83c\udfaf Write to EMPLOYEE_TARGET\nexp_employee_df.write.jdbc(\n    url=oracle_url,\n    table=\"EMPLOYEE_TARGET\",\n    mode=\"overwrite\",  # or \"append\", depending on requirements\n    properties=target_properties\n)\n\n# Stop Spark session\nspark.stop()\n```"}
{"prompt": "Mapping: m_Load_Latest_Account\nSource: ACCOUNT\nTarget: DB_ACCOUNT\nSource Fields:\n- `ID`: integer (10,0)\n- `NAME`: varchar (100,0)\n- `PERSONAL`: varchar (255,0)\n- `LASTMODIFIED_DATE`: datetime (19,0)\nTarget Fields:\n- `ID`: integer (10,0)\n- `NAME`: varchar (100,0)\n- `PERSONAL`: varchar (255,0)\n- `LASTMODIFIED_DATE`: datetime (19,0)\nTransformations:\n- SQL: SELECT MAX(LASTMODIFIED_DATE) AS MAX_DATE FROM ACCOUNT\n- Expression: IIF(LASTMODIFIED_DATE = MAX_DATE, 1, 0)\nConnector Flow:\n- SQ_ACCOUNT`.`ID -> RTR_LATEST_RECORD\n- SQ_ACCOUNT`.`NAME -> RTR_LATEST_RECORD\n- SQ_ACCOUNT`.`PERSONAL -> RTR_LATEST_RECORD\n- SQ_ACCOUNT`.`LASTMODIFIED_DATE -> EXP_COMPARE_DATE\n- LKP_MAX_DATE`.`MAX_DATE -> EXP_COMPARE_DATE\n- SQ_ACCOUNT`.`LASTMODIFIED_DATE -> EXP_COMPARE_DATE\n- EXP_COMPARE_DATE`.`o_IS_LATEST -> RTR_LATEST_RECORD\n- RTR_LATEST_RECORD`.`ID -> DB_ACCOUNT\n- RTR_LATEST_RECORD`.`NAME -> DB_ACCOUNT\n- RTR_LATEST_RECORD`.`PERSONAL -> DB_ACCOUNT\n- RTR_LATEST_RECORD`.`LASTMODIFIED_DATE -> DB_ACCOUNT", "completion": "\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, max as spark_max, when\n\n# Start Spark session\nspark = SparkSession.builder \\\n    .appName(\"m_Load_Latest_Account\") \\\n    .config(\"spark.jars\", \"/path/to/sqljdbc42.jar\") \\  # SQL Server JDBC jar\n    .getOrCreate()\n\n# JDBC connection config\njdbc_url = \"jdbc:sqlserver://hostname:port;databaseName=yourDB\"\nconnection_properties = {\n    \"user\": \"your_user\",\n    \"password\": \"your_password\",\n    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n\n# \ud83d\udce5 Step 1: Read source table\naccount_df = spark.read.jdbc(\n    url=jdbc_url,\n    table=\"ACCOUNT\",\n    properties=connection_properties\n)\n\n# \ud83d\udd0d Step 2: Lookup max LASTMODIFIED_DATE\nmax_date = account_df.agg(spark_max(\"LASTMODIFIED_DATE\").alias(\"MAX_DATE\")).collect()[0][\"MAX_DATE\"]\n\n# \ud83e\udde0 Step 3: Expression transformation (EXP_COMPARE_DATE)\naccount_with_flag_df = account_df.withColumn(\n    \"o_IS_LATEST\",\n    when(col(\"LASTMODIFIED_DATE\") == max_date, 1).otherwise(0)\n)\n\n# \ud83d\udd00 Step 4: Router (RTR_LATEST_RECORD) - Filter for latest records\nlatest_account_df = account_with_flag_df.filter(col(\"o_IS_LATEST\") == 1)\n\n# \ud83d\udce4 Step 5: Write to target table\nlatest_account_df.select(\"ID\", \"NAME\", \"PERSONAL\", \"LASTMODIFIED_DATE\").write.jdbc(\n    url=jdbc_url,\n    table=\"DB_ACCOUNT\",\n    mode=\"overwrite\",  # Or \"append\" based on use case\n    properties=connection_properties\n)\n\n# Stop Spark session\nspark.stop()\n```"}
